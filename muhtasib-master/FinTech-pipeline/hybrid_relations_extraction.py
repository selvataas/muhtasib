#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Proje: 5411 â€“ Relation Types Ã‡Ä±karma (Hybrid: Embedding + LLM)
AmaÃ§: Embedding ile benzer maddeleri bul, LLM'e sadece ilgili maddeleri gÃ¶nder
"""

import os
import json
import csv
import psycopg2
import google.generativeai as genai
import time
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any, Tuple
from tqdm import tqdm


# ======================================================
# Config
# ======================================================
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
SIMILARITY_THRESHOLD = 0.92  
TOP_K_SIMILAR = 5  


# ======================================================
#  DB Connection
# ======================================================

def get_db_connection():
    """PostgreSQL baÄŸlantÄ±sÄ± kur"""
    conn = psycopg2.connect(
        host="192.168.81.57",
        port=5433,
        database="mevzuat",
        user="stas",
        password="MGmnAWJQGaxrAZHK"
    )
    conn.set_session(autocommit=True)
    return conn


def get_article_text_by_number(article_no: str) -> str:
    """5411 sayÄ±lÄ± Kanunun ilgili madde metnini dÃ¶ndÃ¼r"""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT v.content
        FROM articles a
        LEFT JOIN versionarticle v ON a.id = v.article_id
        WHERE a.law_id = 596
          AND CAST(a.number AS TEXT) = %s
          AND (a.abolition IS NULL OR a.abolition = false)
          AND (a.temporary IS NULL OR a.temporary = false);
    """, (article_no,))
    row = cursor.fetchone()
    cursor.close()
    conn.close()
    return row[0] if row and row[0] else ""


# ======================================================
#  CU Load
# ======================================================

def load_compliance_units(cu_file_path: str) -> List[Dict[str, Any]]:
    with open(cu_file_path, "r", encoding="utf-8") as f:
        cu_list = json.load(f)
    return [cu for cu in cu_list if cu["cu_id"].startswith("CU_5411_")]


def parse_article_number_from_cu_id(cu_id: str) -> str:
    """CU_5411_22/1 â†’ 22"""
    try:
        return cu_id.split("_", 2)[2].split("/")[0]
    except Exception:
        return None


# ======================================================
#  EMBEDDING MODEL
# ======================================================

class EmbeddingAnalyzer:
    def __init__(self, model_name=EMBEDDING_MODEL):
        print(f"ğŸ¤– Embedding modeli yÃ¼kleniyor: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.model_name = model_name
        self.cache_file = f"embeddings_5411_articles_{model_name.replace('/', '_')}.pkl"
    
    def create_article_embeddings(self, article_texts: Dict[str, str]) -> Dict[str, np.ndarray]:
        """Madde metinleri iÃ§in embedding oluÅŸtur"""
        
        if os.path.exists(self.cache_file):
            print(f" Cache'den yÃ¼kleniyor: {self.cache_file}")
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)
                if set(article_texts.keys()).issubset(set(cache_data.keys())):
                    return {k: cache_data[k] for k in article_texts.keys()}
        
        print(" Madde embeddings oluÅŸturuluyor...")
        embeddings = {}
        
        article_list = list(article_texts.items())
        texts = [text for _, text in article_list]
        
        encoded = self.model.encode(texts, show_progress_bar=True, batch_size=8)
        
        for i, (article_no, _) in enumerate(article_list):
            embeddings[article_no] = encoded[i]
        
        with open(self.cache_file, 'wb') as f:
            pickle.dump(embeddings, f)
        print(f" Cache kaydedildi: {self.cache_file}")
        
        return embeddings
    
    def find_similar_articles(self, article_no: str, embeddings: Dict[str, np.ndarray], 
                            top_k: int = TOP_K_SIMILAR, threshold: float = SIMILARITY_THRESHOLD) -> List[Tuple[str, float]]:
        """Bir maddeye benzer maddeleri bul"""
        
        if article_no not in embeddings:
            return []
        
        target_embedding = embeddings[article_no].reshape(1, -1)
        similar_articles = []
        
        for other_article, other_embedding in embeddings.items():
            if other_article == article_no:
                continue
            
            similarity = cosine_similarity(target_embedding, other_embedding.reshape(1, -1))[0][0]
            
            if similarity >= threshold:
                similar_articles.append((other_article, float(similarity)))
        
        similar_articles.sort(key=lambda x: x[1], reverse=True)
        return similar_articles[:top_k]


# ======================================================
# GEMINI MODEL
# ======================================================

def setup_llm():
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError(" GEMINI_API_KEY ortam deÄŸiÅŸkenini ayarla!")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("models/gemini-2.5-pro")


# ======================================================
#  PROMPT TEMPLATE
# ======================================================

PROMPT_TEMPLATE = """
Proje Promptu: 5411 â€“ Relation Types Ã‡Ä±karma (Hybrid Analiz)

AÅŸaÄŸÄ±daki gÃ¶revi uygula. TÃ¼rkÃ§e yanÄ±t ver.
Ã‡Ä±ktÄ±yÄ±, Excel'e doÄŸrudan kopyalanabilecek, tab-separated formatta Ã¼ret.

Rol:
Finansal uyum uzmanÄ± bir hukuk analisti gibi Ã§alÄ±ÅŸ. 5411 sayÄ±lÄ± Kanun maddeleri arasÄ±nda (ve gerektiÄŸinde diÄŸer kanun/maddelere) iliÅŸki (relation) etiketlemesi yap.

Ä°liÅŸki Tipleri:
- refer (Reference): Kaynak madde/fÄ±kra, yorum/uygulama iÃ§in baÅŸka bir madde/fÄ±kra veya kanuna atÄ±f yapÄ±yor (bilgi/definisyona ihtiyaÃ§).
- exclude (Exclusion): Kaynak madde/fÄ±kra koÅŸullarÄ± saÄŸlanÄ±rsa, hedef madde/fÄ±kra uygulanmaz/bertaraf edilir.
- only_include (Exclusive Applicability): Kaynak madde/fÄ±kra koÅŸullarÄ± saÄŸlanÄ±rsa uygulama kapsamÄ± yalnÄ±zca hedef madde/fÄ±kralarla sÄ±nÄ±rlandÄ±rÄ±lÄ±r.
- should_include (Mandatory Inclusion): Kaynak madde/fÄ±kra, hedef madde/fÄ±kranÄ±n kÄ±sÄ±tÄ±na uymayÄ± aÃ§Ä±kÃ§a zorunlu kÄ±lar (genel kuralÄ± veya baÅŸka kuralÄ± baÄŸlayÄ±cÄ± ÅŸekilde devreye sokar).

Not: DiÄŸer dÃ¶rt kategoriye girmeyen, ancak belirtilmesi gereken Ã¶zel durumlar veya notlar iÃ§in kullanÄ±lÄ±r.

Girdi
FÄ±kra TanÄ±mÄ±: Girdi metninde, aralarÄ±nda boÅŸluk bÄ±rakÄ±larak yazÄ±lmÄ±ÅŸ her bir paragraf ayrÄ± bir "fÄ±kra" olarak kabul edilir ve buna gÃ¶re numaralandÄ±rÄ±lÄ±r.

law_no_source: 5411
madde_no: {madde_no}
madde_metni:
{madde_text}

BENZER MADDELER (Embedding ile bulundu):
{similar_articles_context}

Ã‡Ä±ktÄ± FORMATI (Sekmeyle AyrÄ±lmÄ±ÅŸ Metin - TSV):
Her madde iÃ§in fÄ±kra bazÄ±nda analiz yap ve sonucu aÅŸaÄŸÄ±daki sÃ¼tun yapÄ±sÄ±na uygun, tek bir satÄ±r olarak Ã¼ret. SÃ¼tunlar arasÄ±nda ayraÃ§ olarak sekme (`\\t`) kullan. Her fÄ±kra yeni bir satÄ±rda olmalÄ±dÄ±r.

SÃ¼tunlar:
FÄ±kra No\\tReferences\\tExclude\\tonly include\\tshould include\\tNot

Kurallar:
- Ã‡Ä±ktÄ± dosyasÄ±na baÅŸlÄ±k satÄ±rÄ± ekle.
- Her fÄ±kra iÃ§in tek bir satÄ±r oluÅŸtur.
- SÃ¼tunlar arasÄ±nda ayraÃ§ olarak sadece sekme (`\\t`) kullan.
- References iliÅŸkisi iÃ§in Ã§Ä±ktÄ±yÄ± yapÄ±sal formatta ver: `kanun_no: [Numara] ([AdÄ±]), madde_no: [Numara], fÄ±kra_no: [Numara]`. Metinde belirtilmeyen alanlara "belirtilmemiÅŸtir" yaz.
- DiÄŸer sÃ¼tunlarda bulgu yoksa "Yok" yaz.
- BENZER MADDELER bÃ¶lÃ¼mÃ¼nÃ¼ dikkate al: EÄŸer madde metni bu maddelere atÄ±f yapÄ±yorsa, iliÅŸkileri belirt.

Ã–NEMLÄ° KISITLAMALAR - Ã‡OK KATIYIZ:
- SADECE metinde AÃ‡IKÃ‡A ve DOÄRUDAN belirtilen iliÅŸkileri yaz.
- Yoruma dayalÄ±, dolaylÄ± veya Ã§Ä±karÄ±m gerektiren iliÅŸkileri ASLA EKLEME.
- "References": SADECE metinde madde/fÄ±kra numarasÄ± GEÃ‡Ä°YORSA yaz (Ã¶rn: "8 inci madde", "23. madde").
- "Exclude": SADECE "...uygulanmaz", "...hariÃ§", "...istisna" gibi AÃ‡IK ifadeler varsa.
- "only_include": SADECE "...ile sÄ±nÄ±rlÄ±", "...sadece" gibi AÃ‡IK kÄ±sÄ±tlama varsa.
- "should_include": SADECE "...uygulanÄ±r", "...tabi olur" gibi AÃ‡IK zorunluluk varsa.
- "Not": Neredeyse HÄ°Ã‡BÄ°R ZAMAN kullanma. Sadece Ã§ok Ã¶zel durumlar iÃ§in.
- ÅÃ¼phe varsa MUTLAKA "Yok" yaz.
- HEDEF: Ã‡oÄŸu fÄ±krada sadece "Yok" olmalÄ±. Bu NORMALDIR ve BEKLENENDÄ°R.

Ã–rnek Ã§Ä±ktÄ± biÃ§imi (SADECE AÃ‡IK Ä°LÄ°ÅKÄ°LER):
FÄ±kra No\\tReferences\\tExclude\\tonly include\\tshould include\\tNot
1\\tkanun_no: 5411, madde_no: 8, fÄ±kra_no: 1, bent_no: a,b,c,d\\tYok\\tYok\\tYok\\tYok
2\\tkanun_no: 5411, madde_no: 23, fÄ±kra_no: 1\\tYok\\tYok\\tYok\\tYok
3\\tYok\\tYok\\tYok\\tYok\\tYok
4\\tYok\\tYok\\tYok\\tYok\\tYok

NOT: Ã‡oÄŸu fÄ±krada sadece "References" veya hiÃ§ iliÅŸki olmayacak. Bu NORMAL ve BEKLENENDÄ°R.
"""


# ======================================================
#  HYBRID ANALYSIS
# ======================================================

def analyze_article_with_hybrid(model, embedding_analyzer, article_no: str, madde_text: str, 
                                embeddings: Dict[str, np.ndarray]) -> str:
    """Hybrid: Embedding ile benzer maddeleri bul, LLM'e context olarak gÃ¶nder"""
    
    similar_articles = embedding_analyzer.find_similar_articles(article_no, embeddings)
    
    similar_context = ""
    if similar_articles:
        similar_context = "Embedding analizi ile bulunan benzer maddeler:\n"
        for sim_article, sim_score in similar_articles:
            similar_context += f"- Madde {sim_article} (benzerlik: {sim_score:.3f})\n"
    else:
        similar_context = "Bu maddeye benzer baÅŸka madde bulunamadÄ± (embedding threshold: {})".format(SIMILARITY_THRESHOLD)
    
    
    prompt = PROMPT_TEMPLATE.format(
        madde_no=article_no,
        madde_text=madde_text.strip(),
        similar_articles_context=similar_context
    )
    
    try:
        response = model.generate_content(prompt)
        output_text = response.text.strip()
        tqdm.write(f"   Madde {article_no} iÅŸlendi (benzer: {len(similar_articles)} madde)")
        return output_text
    except Exception as e:
        tqdm.write(f"   [HATA] Madde {article_no} iÅŸlenemedi: {e}")
        return ""


# ======================================================
#  OUTPUT FORMATTERS
# ======================================================

def parse_tsv_line(tsv_line: str) -> Dict[str, str]:
    """TSV satÄ±rÄ±nÄ± parse et ve dictionary'ye Ã§evir"""
    parts = tsv_line.split('\t')
    if len(parts) >= 6:
        return {
            'fikra_no': parts[0].strip(),
            'references': parts[1].strip(),
            'exclude': parts[2].strip(),
            'only_include': parts[3].strip(),
            'should_include': parts[4].strip(),
            'not': parts[5].strip()
        }
    return {}


def save_as_tsv(results: List[Dict], output_path: str):
    """SonuÃ§larÄ± TSV formatÄ±nda kaydet"""
    print(f"\n TSV dosyasÄ± oluÅŸturuluyor: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f, delimiter='\t')
        
        writer.writerow(['Madde No', 'CU IDs', 'FÄ±kra No', 'References', 'Exclude', 
                        'Only Include', 'Should Include', 'Not'])
        
        for result in results:
            madde_no = result['madde_no']
            cu_ids = ', '.join(result['cu_ids'])
            tsv_output = result['tsv_output']
            
            # TSV Ã§Ä±ktÄ±sÄ±nÄ± satÄ±rlara bÃ¶l
            lines = tsv_output.strip().split('\n')
            for line in lines:
                # BaÅŸlÄ±k satÄ±rÄ±nÄ± atla
                if line.startswith('FÄ±kra No'):
                    continue
                if line.strip():
                    parsed = parse_tsv_line(line)
                    if parsed:
                        writer.writerow([
                            madde_no,
                            cu_ids,
                            parsed.get('fikra_no', ''),
                            parsed.get('references', ''),
                            parsed.get('exclude', ''),
                            parsed.get('only_include', ''),
                            parsed.get('should_include', ''),
                            parsed.get('not', '')
                        ])
    
    print(f" TSV dosyasÄ± kaydedildi: {output_path}")


def save_as_csv(results: List[Dict], output_path: str):
    """SonuÃ§larÄ± CSV formatÄ±nda kaydet"""
    print(f"\n CSV dosyasÄ± oluÅŸturuluyor: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        
        writer.writerow(['Madde No', 'CU IDs', 'FÄ±kra No', 'References', 'Exclude', 
                        'Only Include', 'Should Include', 'Not'])
        
        for result in results:
            madde_no = result['madde_no']
            cu_ids = ', '.join(result['cu_ids'])
            tsv_output = result['tsv_output']
            
            lines = tsv_output.strip().split('\n')
            for line in lines:
                if line.startswith('FÄ±kra No'):
                    continue
                if line.strip():
                    parsed = parse_tsv_line(line)
                    if parsed:
                        writer.writerow([
                            madde_no,
                            cu_ids,
                            parsed.get('fikra_no', ''),
                            parsed.get('references', ''),
                            parsed.get('exclude', ''),
                            parsed.get('only_include', ''),
                            parsed.get('should_include', ''),
                            parsed.get('not', '')
                        ])
    
    print(f" CSV dosyasÄ± kaydedildi: {output_path}")


def save_as_json(results: List[Dict], output_path: str):
    """SonuÃ§larÄ± JSON formatÄ±nda kaydet"""
    print(f"\n JSON dosyasÄ± oluÅŸturuluyor: {output_path}")
    
    json_data = []
    
    for result in results:
        madde_no = result['madde_no']
        cu_ids = result['cu_ids']
        tsv_output = result['tsv_output']
        madde_text = result['madde_text']
        
        madde_obj = {
            'madde_no': madde_no,
            'cu_ids': cu_ids,
            'madde_text': madde_text,
            'fikralar': []
        }
        
        lines = tsv_output.strip().split('\n')
        for line in lines:
            if line.startswith('FÄ±kra No'):
                continue
            if line.strip():
                parsed = parse_tsv_line(line)
                if parsed:
                    madde_obj['fikralar'].append(parsed)
        
        json_data.append(madde_obj)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f" JSON dosyasÄ± kaydedildi: {output_path}")


# ======================================================
#  MAIN FUNCTION
# ======================================================

def run_5411_hybrid_relation_extraction(cu_file_path: str, output_path: str):
    print(" CU listesi yÃ¼kleniyor...")
    cu_list = load_compliance_units(cu_file_path)
    print(f" {len(cu_list)} CU bulundu.")

    article_to_cus = {}
    for cu in cu_list:
        cu_id = cu["cu_id"]
        article_no = parse_article_number_from_cu_id(cu_id)
        if not article_no:
            continue
        if article_no not in article_to_cus:
            article_to_cus[article_no] = []
        article_to_cus[article_no].append(cu_id)

    print(f" {len(article_to_cus)} benzersiz madde bulundu.")

    print(" Madde metinleri yÃ¼kleniyor...")
    article_texts = {}
    for article_no in article_to_cus.keys():
        madde_text = get_article_text_by_number(article_no)
        if madde_text.strip():
            article_texts[article_no] = madde_text
        else:
            print(f" Madde {article_no} metni boÅŸ, atlanÄ±yor.")

    print(f" {len(article_texts)} madde metni yÃ¼klendi.")

    print(" Embedding analizi baÅŸlÄ±yor...")
    embedding_analyzer = EmbeddingAnalyzer(EMBEDDING_MODEL)
    embeddings = embedding_analyzer.create_article_embeddings(article_texts)
    print(f" {len(embeddings)} madde iÃ§in embedding oluÅŸturuldu.")


    print("\n Benzerlik Analizi Ã–zeti:")
    total_similar = 0
    article_keys = sorted(embeddings.keys(), key=lambda x: int(x))
    
    for article_no in tqdm(article_keys, desc="ğŸ” Benzerlik hesaplanÄ±yor", unit="madde"):
        similar = embedding_analyzer.find_similar_articles(article_no, embeddings)
        if similar:
            total_similar += len(similar)
            tqdm.write(f"  Madde {article_no}: {len(similar)} benzer madde")
    
    print(f"\n Toplam {total_similar} benzerlik iliÅŸkisi bulundu.")
    print(f" LLM'e her madde iÃ§in ortalama {total_similar/len(embeddings):.1f} benzer madde context'i gÃ¶nderilecek.\n")

    print(" Gemini modeli yÃ¼kleniyor...")
    model = setup_llm()
    print(" Model hazÄ±r.")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    article_list = sorted([a for a in article_to_cus.keys() if a in article_texts], key=lambda x: int(x))
    
    print(f"\n Hybrid analiz baÅŸlÄ±yor ({len(article_list)} madde)...")
    
    all_results = []
    
    with open(output_path, "w", encoding="utf-8") as out:
        for article_no in tqdm(article_list, desc="ğŸ“Š Maddeler iÅŸleniyor", unit="madde"):
            cu_ids = article_to_cus[article_no]
            madde_text = article_texts[article_no]

            tsv_output = analyze_article_with_hybrid(
                model, embedding_analyzer, article_no, madde_text, embeddings
            )
            
            if tsv_output:
                out.write(f"\n### Madde {article_no} (Ä°lgili CU'lar: {', '.join(cu_ids)})\n")
                out.write(tsv_output + "\n")
                
                # SonuÃ§larÄ± parse et ve sakla
                all_results.append({
                    'madde_no': article_no,
                    'cu_ids': cu_ids,
                    'tsv_output': tsv_output,
                    'madde_text': madde_text
                })

            time.sleep(1.0)

    base_path = output_path.rsplit('.', 1)[0]
    tsv_path = f"{base_path}.tsv"
    save_as_tsv(all_results, tsv_path)
    
    csv_path = f"{base_path}.csv"
    save_as_csv(all_results, csv_path)
    
    json_path = f"{base_path}.json"
    save_as_json(all_results, json_path)

    print(f"\n Ä°ÅŸlem tamamlandÄ±!")
    print(f" TXT Ã§Ä±ktÄ±sÄ±: {output_path}")
    print(f" TSV Ã§Ä±ktÄ±sÄ±: {tsv_path}")
    print(f" CSV Ã§Ä±ktÄ±sÄ±: {csv_path}")
    print(f" JSON Ã§Ä±ktÄ±sÄ±: {json_path}")


# ======================================================
#  RUN BLOCK
# ======================================================

if __name__ == "__main__":
    run_5411_hybrid_relation_extraction(
        cu_file_path="/Users/selva/Downloads/cu/muhtesib/data/compliance_unit.json",
        output_path="/Users/selva/Downloads/cu/muhtesib/FinTech-pipeline/5411_relation_types_hybrid.txt"
    )

